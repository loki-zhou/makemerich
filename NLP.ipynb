{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    tid1  tid2                          title1_zh                  title2_zh  \\\nid                                                                             \n0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……   \n\n                                            title1_en  \\\nid                                                      \n0   There are two new old-age insurance benefits f...   \n3   \"If you do not come to Shenzhen, sooner or lat...   \n1   \"If you do not come to Shenzhen, sooner or lat...   \n\n                                            title2_en      label  \nid                                                                \n0   Police disprove \"bird's nest congress each per...  unrelated  \n3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n1   The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tid1</th>\n      <th>tid2</th>\n      <th>title1_zh</th>\n      <th>title2_zh</th>\n      <th>title1_en</th>\n      <th>title2_en</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n      <td>There are two new old-age insurance benefits f...</td>\n      <td>Police disprove \"bird's nest congress each per...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>3</td>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4</td>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n      <td>unrelated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\n",
    "    \"./train.csv\", index_col=0)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                            title1_zh                  title2_zh      label\nid                                                                         \n0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated\n3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated\n1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……  unrelated",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title1_zh</th>\n      <th>title2_zh</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n      <td>unrelated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['title1_zh',\n",
    "        'title2_zh',\n",
    "        'label']\n",
    "train = train.loc[:, cols]\n",
    "train.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['I', 'am', 'Meng', 'Lee,', 'a', 'data', 'scientist', 'based', 'in', 'Tokyo.']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I am Meng Lee, a data scientist based in Tokyo.'\n",
    "words = text.split(' ')\n",
    "words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting jieba\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "     ---------------------------------------- 0.0/19.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/19.2 MB 1.4 MB/s eta 0:00:15\n",
      "     --------------------------------------- 0.0/19.2 MB 991.0 kB/s eta 0:00:20\n",
      "     --------------------------------------- 0.1/19.2 MB 787.7 kB/s eta 0:00:25\n",
      "     ---------------------------------------- 0.2/19.2 MB 1.3 MB/s eta 0:00:15\n",
      "      --------------------------------------- 0.4/19.2 MB 2.0 MB/s eta 0:00:10\n",
      "      --------------------------------------- 0.4/19.2 MB 2.0 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.5/19.2 MB 1.6 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 0.6/19.2 MB 1.7 MB/s eta 0:00:11\n",
      "     - -------------------------------------- 0.8/19.2 MB 2.1 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 1.0/19.2 MB 2.2 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 1.0/19.2 MB 2.3 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 1.2/19.2 MB 2.1 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 1.6/19.2 MB 2.6 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 1.7/19.2 MB 2.7 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 1.8/19.2 MB 2.6 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 2.3/19.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 2.3/19.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 2.6/19.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 2.7/19.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 2.7/19.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 2.7/19.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 3.3/19.2 MB 3.4 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 3.3/19.2 MB 3.4 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 3.8/19.2 MB 3.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 4.1/19.2 MB 3.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 4.2/19.2 MB 3.6 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 4.5/19.2 MB 3.7 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 4.7/19.2 MB 3.7 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 4.8/19.2 MB 3.7 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 4.8/19.2 MB 3.7 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 5.7/19.2 MB 4.0 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 5.9/19.2 MB 4.0 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 6.0/19.2 MB 3.9 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 6.0/19.2 MB 3.9 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 6.7/19.2 MB 4.1 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 6.9/19.2 MB 4.2 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 6.9/19.2 MB 4.2 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 6.9/19.2 MB 4.2 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 7.4/19.2 MB 4.1 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 7.5/19.2 MB 4.0 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 7.6/19.2 MB 4.0 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 7.6/19.2 MB 4.0 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 8.6/19.2 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 8.7/19.2 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 8.8/19.2 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 9.1/19.2 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 9.1/19.2 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 9.3/19.2 MB 4.2 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 9.8/19.2 MB 4.3 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 9.9/19.2 MB 4.3 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 9.9/19.2 MB 4.2 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 10.4/19.2 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 10.7/19.2 MB 4.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 10.7/19.2 MB 4.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 10.7/19.2 MB 4.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 10.8/19.2 MB 4.6 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 10.8/19.2 MB 4.6 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 10.8/19.2 MB 4.6 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.2/19.2 MB 4.5 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.3/19.2 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 11.4/19.2 MB 3.1 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 11.4/19.2 MB 3.1 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 11.4/19.2 MB 3.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 11.4/19.2 MB 3.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 11.5/19.2 MB 3.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 11.6/19.2 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 11.6/19.2 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 11.6/19.2 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 11.9/19.2 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 12.0/19.2 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 12.1/19.2 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 12.4/19.2 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 12.6/19.2 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 12.6/19.2 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 12.8/19.2 MB 2.8 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 13.0/19.2 MB 2.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 13.1/19.2 MB 2.8 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 13.3/19.2 MB 2.8 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 13.6/19.2 MB 2.8 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 14.0/19.2 MB 2.8 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 14.2/19.2 MB 2.8 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 14.3/19.2 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 14.6/19.2 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 14.8/19.2 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 14.8/19.2 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 15.0/19.2 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 15.3/19.2 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 15.3/19.2 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 15.6/19.2 MB 2.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 15.8/19.2 MB 2.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 15.8/19.2 MB 2.7 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 16.2/19.2 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 16.3/19.2 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 16.5/19.2 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 16.6/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 16.6/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 17.1/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 17.3/19.2 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 17.4/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 17.6/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 17.6/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 17.6/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 18.0/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 18.0/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 18.0/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 18.5/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  18.8/19.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  19.0/19.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  19.1/19.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  19.2/19.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  19.2/19.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  19.2/19.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 19.2/19.2 MB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314474 sha256=e7af0b465f9efa7ab6aa8626c5c32ac71b5c47a711494cf83667d313cc384517\n",
      "  Stored in directory: c:\\users\\zhouyi\\appdata\\local\\pip\\cache\\wheels\\b2\\9b\\80\\7537177f75993c29af08e0d00c753724c7f06c646352be50a3\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\zhouyi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.763 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[pair('我', 'r'),\n pair('是', 'v'),\n pair('李孟', 'nr'),\n pair('，', 'x'),\n pair('在', 'p'),\n pair('東京', 'ns'),\n pair('工作', 'vn'),\n pair('的', 'uj'),\n pair('數據', 'n'),\n pair('科學家', 'n')]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "text = '我是李孟，在東京工作的數據科學家'\n",
    "words = pseg.cut(text)\n",
    "[word for word in words]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def jieba_tokenizer(text):\n",
    "    text = str(text)\n",
    "    words = pseg.cut(text)\n",
    "    return ' '.join([\n",
    "        word for word, flag in words if flag != 'x'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train['title2_tokenized'] = \\\n",
    "    train.loc[:, 'title2_zh'] \\\n",
    "         .apply(jieba_tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train['title1_tokenized'] = \\\n",
    "    train.loc[:, 'title1_zh'] \\\n",
    "         .apply(jieba_tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                            title1_zh                      title2_tokenized\nid                                                                         \n0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗     警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京\n3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港    深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小\n1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港              GDP 首 超 香港 深圳 澄清 还 差 一点点\n2   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿\n9                \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油     吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title1_zh</th>\n      <th>title2_tokenized</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n      <td>警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>GDP 首 超 香港 深圳 澄清 还 差 一点点</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n      <td>吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, [0, 3]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "                     title2_zh  \\\nid                               \n0     警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n3    深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n1         GDP首超香港？深圳澄清：还差一点点……   \n2   去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n9      吃了30年食用油才知道，一片大蒜轻松鉴别地沟油   \n\n                                   title1_tokenized  \nid                                                   \n0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗  \n3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n2   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n9                        用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title2_zh</th>\n      <th>title1_tokenized</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, [1, 4]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "['狐狸', '被', '陌生人', '拍照']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '狐狸被陌生人拍照'\n",
    "words = pseg.cut(text)\n",
    "words = [w for w, f in words]\n",
    "words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{'狐狸': 0, '被': 1, '陌生人': 2, '拍照': 3}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = {\n",
    "    word: idx\n",
    "    for idx, word in enumerate(words)\n",
    "}\n",
    "word_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['狐狸', '被', '陌生人', '拍照']\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(words)\n",
    "print([word_index[w] for w in words])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['陌生人', '被', '狐狸', '拍照']\n",
      "[2, 1, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "text = '陌生人被狐狸拍照'\n",
    "words = pseg.cut(text)\n",
    "words = [w for w, f in words]\n",
    "print(words)\n",
    "print([word_index[w] for w in words])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: absl-py in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "id\n0                2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n3         你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n1         你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n2         你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n9                              用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n                               ...                       \n321185       萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大\n321182       萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大\n321184       萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大\n321186       萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大\n321188          萨达姆 此项 计划 没有 此国 破坏 的话 美国 还 会 对 伊拉克 发动战争 吗\nName: title1_tokenized, Length: 320552, dtype: object"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "id\n0                2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n3         你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n1         你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n2         你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n9                              用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n                               ...                       \n321185                   西媒 曝 萨拉 赫 被 推荐 至 巴萨 经纪人 辟谣 并未 发生\n321182                            萨拉 赫 经纪人 辟谣 与 巴萨 传闻 不 实\n321184                 萨拉 赫 辟谣 传闻 埃及 非常 团结 我们 之间 没有 任何 分歧\n321186                   辟谣 不 实 传闻 埃及 足协 主席 萨拉 赫 不会 提前 离队\n321188                         萨达姆 女儿 辟谣 萨达姆 政权 二号 人物 没 死\nLength: 641104, dtype: object"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x2 = train.title2_tokenized\n",
    "corpus = pd.concat([\n",
    "    corpus_x1, corpus_x2])\n",
    "corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              title\nid                                                 \n0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n2   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n9                        用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(corpus.iloc[:5],\n",
    "             columns=['title'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(corpus)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "x1_train = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x1)\n",
    "x2_train = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗']\n"
     ]
    }
   ],
   "source": [
    "for seq in x1_train[:1]:\n",
    "    print([tokenizer.index_word[idx] for idx in seq])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 [217, 1268, 32, 1178, 5967]  ...\n",
      "19 [4, 10, 47, 678, 2558]  ...\n",
      "19 [4, 10, 47, 678, 2558]  ...\n",
      "19 [4, 10, 47, 678, 2558]  ...\n",
      "9 [31, 320, 3372, 3062, 1]  ...\n",
      "19 [4, 10, 47, 678, 2558]  ...\n",
      "6 [7, 2221, 1, 2072, 7]  ...\n",
      "19 [4, 10, 47, 678, 2558]  ...\n",
      "14 [1281, 1211, 427, 3, 3245]  ...\n",
      "9 [31, 320, 3372, 3062, 1]  ...\n"
     ]
    }
   ],
   "source": [
    "for seq in x1_train[:10]:\n",
    "    print(len(seq), seq[:5], ' ...')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "61"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max([\n",
    "    len(seq) for seq in x1_train])\n",
    "max_seq_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import keras\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "x1_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_train,\n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "x2_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_train,\n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[   0,    0,    0,    0,    0,    0,  217, 1268,   32, 1178, 5967,\n          25,  489, 2877,  116, 5559,    4, 1850,    2,   13],\n       [   0,    4,   10,   47,  678, 2558,    4,  166,   34,   17,   47,\n        5150,   63,   15,  678, 4502, 3211,   23,  284, 1181],\n       [   0,    4,   10,   47,  678, 2558,    4,  166,   34,   17,   47,\n        5150,   63,   15,  678, 4502, 3211,   23,  284, 1181],\n       [   0,    4,   10,   47,  678, 2558,    4,  166,   34,   17,   47,\n        5150,   63,   15,  678, 4502, 3211,   23,  284, 1181],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          31,  320, 3372, 3062,    1,   95,   98, 3372, 3062]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_train[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 1.], dtype=float32)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定義每一個分類對應到的索引數字\n",
    "label_to_index = {\n",
    "    'unrelated': 0,\n",
    "    'agreed': 1,\n",
    "    'disagreed': 2\n",
    "}\n",
    "\n",
    "# 將分類標籤對應到剛定義的數字\n",
    "y_train = train.label.apply(\n",
    "    lambda x: label_to_index[x])\n",
    "\n",
    "y_train = np.asarray(y_train) \\\n",
    "            .astype('float32')\n",
    "\n",
    "y_train[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_train)\n",
    "\n",
    "y_train[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from sklearn.model_selection \\\n",
    "    import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "# 小彩蛋\n",
    "RANDOM_STATE = 9527\n",
    "\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        x1_train, x2_train, y_train,\n",
    "        test_size=VALIDATION_RATIO,\n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (288496, 20)\n",
      "x2_train: (288496, 20)\n",
      "y_train : (288496, 3)\n",
      "----------\n",
      "x1_val:   (32056, 20)\n",
      "x2_val:   (32056, 20)\n",
      "y_val :   (32056, 3)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新聞標題 1: \n",
      "['', '', '', '', '', '营养师', '补充', '这', '4', '种', '营养', '能', '帮', '你', '降血压', '你', '一样', '都', '不吃', '么']\n",
      "\n",
      "新聞標題 2: \n",
      "['', '', '', '', '', '', '', '', '', '刘涛', '现场', '痛哭', '发飙', '要', '离婚', '直接', '把', '旁边', '的', '了']\n",
      "\n",
      "新聞標題 3: \n",
      "['', '', '', '', '', '', '', '', '', '', '', 'nba', '被', '球星', '诞生', '火箭', '骑士', '同', '抢', '交易']\n",
      "\n",
      "新聞標題 4: \n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '海口', '飞机', '撒药治', '白蛾']\n",
      "\n",
      "新聞標題 5: \n",
      "['', '', '', '', '', '', '', '', '网', '曝', '杜', '海涛', '与', '沈梦辰', '已', '分手', '疑似', '女方', '劈', '腿']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(x1_train[:5]):\n",
    "    print(f\"新聞標題 {i + 1}: \")\n",
    "    print([tokenizer.index_word.get(idx, '') for idx in seq])\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "NUM_EMBEDDING_DIM = 3\n",
    "embedding_layer = layers.Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# 基本參數設置，有幾個分類\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# 在語料庫裡有多少詞彙\n",
    "MAX_NUM_WORDS = 10000\n",
    "\n",
    "# 一個標題最長有幾個詞彙\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "\n",
    "# 一個詞向量的維度\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "\n",
    "# LSTM 輸出的向量維度\n",
    "NUM_LSTM_UNITS = 128"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# 建立孿生 LSTM 架構（Siamese LSTM）\n",
    "from keras import Input\n",
    "from keras.layers import Embedding, \\\n",
    "    LSTM, concatenate, Dense\n",
    "from keras.models import Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# 分別定義 2 個新聞標題 A & B 為模型輸入\n",
    "# 兩個標題都是一個長度為 20 的數字序列\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),\n",
    "    dtype='int32')\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),\n",
    "    dtype='int32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# 詞嵌入層\n",
    "# 經過詞嵌入層的轉換，兩個新聞標題都變成\n",
    "# 一個詞向量的序列，而每個詞向量的維度\n",
    "# 為 256\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# LSTM 層\n",
    "# 兩個新聞標題經過此層後\n",
    "# 為一個 128 維度向量\n",
    "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# 串接層將兩個新聞標題的結果串接單一向量\n",
    "# 方便跟全連結層相連\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output],\n",
    "    axis=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# 全連接層搭配 Softmax Activation\n",
    "# 可以回傳 3 個成對標題\n",
    "# 屬於各類別的可能機率\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES,\n",
    "    activation='softmax')\n",
    "predictions = dense(merged)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs=[top_input, bm_input],\n",
    "    outputs=predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pydot in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\programdata\\anaconda3\\envs\\makemerich\\lib\\site-packages (from pydot) (3.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(\n",
    "    model,\n",
    "    to_file='model.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=False,\n",
    "    rankdir='LR')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"functional_1\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ -                 │\n│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)        │          \u001B[38;5;34m0\u001B[0m │ -                 │\n│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m, \u001B[38;5;34m256\u001B[0m)   │  \u001B[38;5;34m2,560,000\u001B[0m │ input_layer[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m… │\n│ (\u001B[38;5;33mEmbedding\u001B[0m)         │                   │            │ input_layer_1[\u001B[38;5;34m0\u001B[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (\u001B[38;5;33mLSTM\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │    \u001B[38;5;34m197,120\u001B[0m │ embedding[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n│                     │                   │            │ embedding[\u001B[38;5;34m1\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ lstm[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],       │\n│ (\u001B[38;5;33mConcatenate\u001B[0m)       │                   │            │ lstm[\u001B[38;5;34m1\u001B[0m][\u001B[38;5;34m0\u001B[0m]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001B[38;5;33mDense\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3\u001B[0m)         │        \u001B[38;5;34m771\u001B[0m │ concatenate[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │                   │            │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">771</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,757,891\u001B[0m (10.52 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,757,891</span> (10.52 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m2,757,891\u001B[0m (10.52 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,757,891</span> (10.52 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
