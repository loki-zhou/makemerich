> 神经网络Loss函数, 求梯度, 链式求导的最后一层 $\frac{\partial (Wx + b)}{\partial W} = x^T$, 找了全网的资源，基本都是一笔带过, 莫名其妙就 np.dot(x.t, error)

### 以下是详细的推导过程：

1. **定义函数**：
   设 $f(W) = Wx + b$，其中 $W$ 是一个矩阵，$x$ 和 $b$ 是向量。

2. **目标**：
   我们需要求 $\frac{\partial f(W)}{\partial W}$。

3. **展开函数**：
   $f(W) = Wx + b$ 可以展开为：
   $$
   f(W) = \begin{pmatrix}
   W_{11} & W_{12} & \cdots & W_{1n} \\
   W_{21} & W_{22} & \cdots & W_{2n} \\
   \vdots & \vdots & \ddots & \vdots \\
   W_{m1} & W_{m2} & \cdots & W_{mn}
   \end{pmatrix}
   \begin{pmatrix}
   x_1 \\
   x_2 \\
   \vdots \\
   x_n
   \end{pmatrix}
   + b
   $$
   其中 $W$ 是一个 $m \times n$ 矩阵，$x$ 是一个 $n \times 1$ 向量，$b$ 是一个 $m \times 1$ 向量。

4. **计算 $f(W)$ 的每个元素**：
   $$
   f(W) = \begin{pmatrix}
   \sum_{j=1}^n W_{1j} x_j + b_1 \\
   \sum_{j=1}^n W_{2j} x_j + b_2 \\
   \vdots \\
   \sum_{j=1}^n W_{mj} x_j + b_m
   \end{pmatrix}
   $$

5. **求导**：
   我们需要对 $W$ 的每个元素求导。考虑 $f(W)$ 的第 $i$ 个元素：
   $$
   f_i(W) = \sum_{j=1}^n W_{ij} x_j + b_i
   $$
   对 $W_{ij}$ 求导：
   $$
   \frac{\partial f_i(W)}{\partial W_{ij}} = x_j
   $$

6. **构建梯度矩阵**：
   将所有导数组合成一个矩阵：
   $$
   \frac{\partial f(W)}{\partial W} = \begin{pmatrix}
   x_1 & x_2 & \cdots & x_n \\
   x_1 & x_2 & \cdots & x_n \\
   \vdots & \vdots & \ddots & \vdots \\
   x_1 & x_2 & \cdots & x_n
   \end{pmatrix}
   $$
   这个矩阵的形式是 $x$ 的转置重复 $m$ 次，即：
   $$
   \frac{\partial f(W)}{\partial W} = \begin{pmatrix}
   x^T \\
   x^T \\
   \vdots \\
   x^T
   \end{pmatrix}
   $$

7. **简化表示**：
   由于 $x^T$ 是一个行向量，我们可以将其表示为：
   $$
   \frac{\partial f(W)}{\partial W} = x^T
   $$

因此，最终的推导结果是：
$$
\frac{\partial (Wx + b)}{\partial W} = x^T
$$
